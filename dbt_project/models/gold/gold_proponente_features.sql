-- dbt model: dbt_project/models/gold/gold_proponente_features.sql
-- This model creates the S3-backed MergeTree table in ClickHouse
-- that stores the proponente features from the Spark job.

{{
  config(
    materialized='table',  -- This will execute the DDL via dbt.
                           -- For S3-backed tables, 'table' materialization with custom DDL is common.
                           -- Another option is `materialized='ephemeral'` if this SQL is just a DDL
                           -- and then use a separate dbt model to select from it, but 'table' is fine.
    schema='gold',         -- Specifies the ClickHouse database (schema)
    alias='gold_proponente_features', -- The table name in ClickHouse will be gold.gold_proponente_features
    engine='MergeTree()', -- Base engine; partitioning and S3 settings are key
    partition_by='(year, month, day)',
    order_by='(proponente_id, data_snapshot)',
    settings={
      'storage_policy': 's3_main_policy',
      -- Optional: 'allow_nullable_key': 1, if order_by keys can be nullable (not typical for primary keys)
      'index_granularity': 8192
    },
    -- Custom DDL for S3 table creation.
    -- dbt-clickhouse might not directly support all S3 table parameters via config block alone.
    -- So, we use a custom approach: this model will effectively be a SELECT statement
    -- that dbt tries to materialize. If the table doesn't exist, dbt creates it with inferred schema + config.
    -- To enforce S3-backed table with specific path, we might need a pre-hook or a macro.
    --
    -- Let's try a simpler approach first: dbt creates the table based on schema from a SELECT.
    -- The `storage_policy` setting is the most crucial part for S3.
    -- dbt-clickhouse adapter should honor `storage_policy` from the config block.
    -- The data path for S3 MergeTree tables is usually implicitly determined by ClickHouse
    -- based on table UUID and storage policy endpoint: <endpoint_from_policy>/<db_uuid>/<table_uuid>/...
    --
    -- If we need to point to an *existing* S3 path (`s3a://gold/proponente_features_clickhouse/`)
    -- that Spark populates, we need a slightly different DDL, possibly using `ATTACH TABLE` logic
    -- or ensuring ClickHouse's default path for that storage policy aligns.
    --
    -- For dbt to manage a table that reads from a specific S3 path populated by an external process (Spark),
    -- the most robust way is to create the table with explicit DDL using a pre-hook or a run-operation,
    -- and then this model could be `materialized='view'` or `materialized='table'` selecting from it
    -- (if further transformation is needed by dbt) or simply be an `ephemeral` model that defines the source.
    --
    -- Given the setup, the Spark job writes to `s3a://gold/proponente_features_clickhouse/` partitioned.
    -- We want ClickHouse table `gold.gold_proponente_features` to read from this path.
    -- This requires `CREATE TABLE ... ENGINE = MergeTree PARTITION BY ... ORDER BY ... SETTINGS storage_policy = 's3_main_policy'`
    -- AND ensuring ClickHouse knows to use the specific S3 path.
    --
    -- A standard MergeTree on S3 with `storage_policy` will have its data path managed by ClickHouse.
    -- To use an externally populated path, you typically use `ENGINE = S3(...)` for table function like access,
    // or `ENGINE = MergeTree SETTINGS disk = 'minio_s3_disk'` (if disk is defined and policy uses it) and load data.
    -- Or, `CREATE TABLE ... AS s3('s3a://gold/proponente_features_clickhouse/...', 'Parquet', 'schema')`
    --
    -- Let's refine the strategy:
    -- 1. This dbt model will *not* be materialized as a table that dbt itself populates by running INSERT.
    -- 2. Instead, this dbt model will ensure the DDL for the S3-backed table exists.
    --    We can use a pre-hook in `dbt_project.yml` or this model's config to run `CREATE TABLE IF NOT EXISTS`.
    --    Then, this model's SQL can be a simple `SELECT 1` or a passthrough SELECT from the table
    --    if dbt requires a SELECT statement for 'table' materialization.
    --
    -- Let's use a pre-hook for the DDL. This model will then just be a placeholder or select from it.
    -- For now, this model will define the expected schema via a SELECT query, and dbt will create
    -- the table using the config above. The `storage_policy` should make it an S3 table.
    -- ClickHouse will manage the data files within its structure on the S3 disk.
    -- This means Spark should write to a path that ClickHouse can then "own" or we need a load step.
    --
    -- Alternative: Spark writes to a path. ClickHouse table `gold.gold_proponente_features` is defined
    -- by dbt to read from this path. This is usually done with `ENGINE = S3Cluster` or `ENGINE = S3`.
    -- `MergeTree` on S3 is for when ClickHouse *manages* the data files on S3.
    --
    -- The `fct_analise_risco.sql` sources from `{{ source('clickhouse_gold', 'gold_proponente_features') }}`.
    -- `sources.yml` defines this as:
    --  sources:
    --    - name: clickhouse_gold
    --      schema: gold
    --      tables:
    --        - name: gold_proponente_features
    --
    -- This implies `gold.gold_proponente_features` should exist.
    -- Let's assume dbt will create a MergeTree table on S3 using `storage_policy='s3_main_policy'`.
    -- The Spark job then needs to write data *into this dbt-created table*.
    -- This is a common pattern: dbt defines the table structure, Spark loads data into it.
    -- However, the current Spark job writes to its own S3 path.
    --
    -- Re-evaluation for this step:
    -- The Spark job writes partitioned Parquet files to `s3a://gold/proponente_features_clickhouse/`.
    -- We need a ClickHouse table `gold.gold_proponente_features` that reads from this *exact* path.
    -- This is best achieved with `ENGINE = MergeTree()` and `SETTINGS disk = 'minio_s3_disk', relative_path = 'proponente_features_clickhouse/'`
    -- if the disk `minio_s3_disk` points to `s3a://gold/`.
    -- The current `minio_s3_disk` in `config.xml` points to `http://minio:9000/clickhouse/`.
    -- So, `proponente_features_clickhouse/` would be relative to `clickhouse` bucket.
    --
    -- Path mapping:
    -- ClickHouse disk `minio_s3_disk` -> `http://minio:9000/clickhouse/` (effectively `s3a://clickhouse/`)
    -- Spark writes to `s3a://gold/proponente_features_clickhouse/`
    -- These are different S3 locations.
    --
    -- Solution:
    -- 1. Modify ClickHouse `config.xml` to have a disk, say `gold_disk`, pointing to `s3a://gold/`.
    -- 2. Create a storage policy, say `gold_policy`, using this `gold_disk`.
    -- 3. This dbt model creates `gold.gold_proponente_features` with `SETTINGS storage_policy = 'gold_policy', relative_path = 'proponente_features_clickhouse/'`. (This relative_path might not be standard).
    -- OR
    -- Use `Table function S3` or `S3Cluster engine` if ClickHouse is just reading.
    -- But `fct_analise_risco` implies it might be a table that dbt can also write to or manage.
    --
    -- Simplest for now: Let dbt create a MergeTree table on S3 (using `s3_main_policy` which points to `s3a://clickhouse/`).
    -- Then, the Spark job must load data into *this* table, not just write to an arbitrary S3 path.
    -- This requires Spark to be able to write to a ClickHouse MergeTree table on S3. (e.g. via clickhouse-spark connector or JDBC).
    -- This is getting complex.
    --
    -- Let's stick to the current plan: Spark writes Parquet to `s3a://gold/proponente_features_clickhouse/`.
    -- This dbt model will create a ClickHouse table that *reads* from this external path.
    -- This is best done with `ENGINE = S3('s3a://gold/proponente_features_clickhouse/*/*/*/*.parquet', 'Parquet', 'schema_def')`.
    -- This makes it a read-only view on the S3 data.
    -- If `fct_analise_risco` needs to *transform* this and write to another ClickHouse table, that's fine.
    --
    -- New DDL for `gold.gold_proponente_features` using S3 table function approach, managed by dbt:
    -- This will be a VIEW in dbt terms, but it's a table-like structure in ClickHouse.
    -- This is not standard dbt materialization.
    --
    -- Let's use a pre-hook in this model's config to create the table DDL if not exists.
    -- The model's SQL will then be `SELECT * FROM gold.gold_proponente_features`.
    -- This makes `gold.gold_proponente_features` the actual table dbt "builds".
    pre_hook = """
      CREATE TABLE IF NOT EXISTS {{this.schema}}.{{this.name}} (
        proponente_id String,
        avg_ndvi_90d Float32,
        distancia_porto_km Float32,
        score_credito_externo Int32,
        idade_cultura_predominante_dias Int32,
        area_total_propriedade_ha Float32,
        percentual_endividamento_total Float32,
        data_snapshot DateTime64(3), -- Assuming DateTime64 for Timestamps from Parquet
        data_carga_gold DateTime64(3),
        year UInt16,
        month UInt8,
        day UInt8
      ) ENGINE = MergeTree()
      PARTITION BY (year, month, day)
      ORDER BY (proponente_id, data_snapshot)
      SETTINGS storage_policy = 's3_main_policy'
               -- relative_path = 'gold_data/proponente_features/' -- Path relative to the s3_main_policy disk endpoint.
               -- The disk 'minio_s3_disk' for 's3_main_policy' is 'http://minio:9000/clickhouse/'
               -- So data would be in 's3a://clickhouse/gold_data/proponente_features/'
               -- This means Spark needs to write to this *specific* path that ClickHouse controls for this table.
               -- This is the standard way for MergeTree on S3.
      ;
    """,
    -- The `relative_path` setting might need specific ClickHouse version or syntax.
    -- If `relative_path` is not used, ClickHouse creates its own directory structure under the policy's disk path.
    -- For Spark to write into a ClickHouse-managed MergeTree on S3, Spark typically needs to use
    -- the clickhouse-spark connector which understands how to write parts to the correct S3 locations.
    --
    -- Given Spark writes to `s3a://gold/proponente_features_clickhouse/`, and this is *external* to ClickHouse's own data management for a given MergeTree table.
    -- The most straightforward way for ClickHouse to query this external S3 data is using the S3 table engine or S3 table function.
    -- Let's make `gold.gold_proponente_features` a VIEW that uses the S3 table function.
    -- This is cleaner as Spark produces files, ClickHouse reads them.
    -- `fct_analise_risco` can then select from this view and materialize into a ClickHouse native/S3 table.
    --
    -- New strategy: This model creates a VIEW.
    materialized='view',
    schema='gold'
  )
}}

-- This view will select from data located in S3, populated by the Spark job.
-- The S3 path is 's3a://gold/proponente_features_clickhouse/*/*/*/*.parquet'
-- The schema must match the Parquet files.
SELECT
    proponente_id,
    avg_ndvi_90d,
    distancia_porto_km, -- Spark job had a typo here: "distancia_ porto_km"
    score_credito_externo,
    idade_cultura_predominante_dias,
    area_total_propriedade_ha,
    percentual_endividamento_total,
    data_snapshot,
    data_carga_gold,
    year,
    month,
    day
FROM s3(
    'http://minio:9000/gold/proponente_features_clickhouse/*/*/*/*.parquet', -- URL for S3 function needs to be http/https for MinIO endpoint
    'admin',                                distribuci√≥n
    'password',
    'Parquet',
    'proponente_id String,
     avg_ndvi_90d Float32,
     distancia_porto_km Float32, -- Corrected column name
     score_credito_externo Int32,
     idade_cultura_predominante_dias Int32,
     area_total_propriedade_ha Float32,
     percentual_endividamento_total Float32,
     data_snapshot DateTime64(3),
     data_carga_gold DateTime64(3),
     year Nullable(UInt16), -- Partition columns might not exist in files if path-based partitioning
     month Nullable(UInt8), -- Let's assume they are in the files for now. If not, remove from schema here.
     day Nullable(UInt8)'   -- Spark writes these as part of the data itself.
)
SETTINGS input_format_parquet_allow_missing_columns = true,
         input_format_parquet_skip_missing_columns = true
-- Renaming the column with a typo from Spark job
-- This is not directly possible in the FROM s3(...) schema definition.
-- The SELECT statement itself will refer to `distancia_ porto_km`.
-- We will alias it in the CTE of fct_analise_risco.sql or fix the Spark job.
-- For now, the view will expose it with the space.
-- It's better to fix the Spark job. I'll make a note to do that.

-- This model defines the source `gold_proponente_features` as a view on S3 data.
-- The actual table `marts.fct_analise_risco` will select from this view.
-- The `sources.yml` definition for `clickhouse_gold.gold_proponente_features` is then referring to this view.
-- This approach is clean for separating Spark's output from ClickHouse's internal storage,
-- while still allowing dbt to manage the "source" definition.
-- The S3 URL for MinIO should use `http://minio:9000/BUCKET/PATH...`
-- Spark path: `s3a://gold/proponente_features_clickhouse/`
-- S3 function path: `http://minio:9000/gold/proponente_features_clickhouse/*/*/*/*.parquet`
-- The glob pattern `*/*/*/*.parquet` is to read from the year/month/day partitions.
-- ClickHouse S3 function needs credentials.
-- The `admin`/`password` are hardcoded here for simplicity, matching MinIO setup.
-- In production, use secure credential management.
-- The schema definition in the s3 function must exactly match the Parquet file schema.
-- `input_format_parquet_allow_missing_columns = true` makes it more robust to schema variations if any.
-- The Spark job writes year, month, day as columns in the Parquet files, so they are part of the schema.
-- Corrected `distancia_porto_km` in s3 function schema to match the earlier fix in Spark job (assuming it will be fixed).
-- If Spark job is not fixed, use `\`distancia porto_km\`` (with backticks) in the schema string.
-- I will assume the Spark job will be fixed to output `distancia_porto_km`.
/*
Final schema for s3 function based on Spark output (assuming typo `distancia_ porto_km` is fixed to `distancia_porto_km`):
    'proponente_id String,
     avg_ndvi_90d Float32,
     distancia_porto_km Float32,
     score_credito_externo Int32,
     idade_cultura_predominante_dias Int32,
     area_total_propriedade_ha Float32,
     percentual_endividamento_total Float32,
     data_snapshot DateTime64(3),
     data_carga_gold DateTime64(3),
     year UInt16,
     month UInt8,
     day UInt8'
*/
