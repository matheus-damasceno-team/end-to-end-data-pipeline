# Spark default configurations for S3 (MinIO) and Hive Metastore

# S3A (MinIO) Configuration
spark.hadoop.fs.s3a.endpoint                      http://minio:9000
spark.hadoop.fs.s3a.access.key                      admin
spark.hadoop.fs.s3a.secret.key                      password
spark.hadoop.fs.s3a.path.style.access               true
spark.hadoop.fs.s3a.impl                            org.apache.hadoop.fs.s3a.S3AFileSystem
# Optional: For multipart uploader, if dealing with large files (usually enabled by default)
# spark.hadoop.fs.s3a.fast.upload                   true
# spark.hadoop.fs.s3a.multipart.size                67108864 # 64MB
# spark.hadoop.fs.s3a.multipart.threshold           209715200 # 200MB

# Hive Metastore Configuration
spark.sql.catalogImplementation                     hive
spark.hive.metastore.uris                           thrift://hive-metastore:9083

# Iceberg Configuration
spark.jars.packages                                 org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2,org.apache.hadoop:hadoop-aws:3.3.1,com.amazonaws:aws-java-sdk-bundle:1.11.901
spark.sql.extensions                                org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
spark.sql.catalog.hive_catalog                      org.apache.iceberg.spark.SparkSessionCatalog
spark.sql.catalog.hive_catalog.type                 hive
spark.sql.catalog.hive_catalog.uri                  thrift://hive-metastore:9083
spark.sql.catalog.hive_catalog.warehouse            s3a://warehouse/iceberg_data
spark.sql.defaultCatalog                            hive_catalog
# Optional: If Hive warehouse is on S3 (MinIO) - this is where Spark managed tables would go by default.
# spark.sql.warehouse.dir                           s3a://warehouse/spark-warehouse/
# Ensure the 'warehouse' bucket exists in MinIO if you use this. setup.sh creates it.

# For Parquet schema merging and evolution (optional but often useful)
spark.sql.parquet.mergeSchema                       true
# spark.sql.parquet.writeLegacyFormat               false # Use modern Parquet format

# Other potentially useful Spark configurations
# spark.serializer                                  org.apache.spark.serializer.KryoSerializer
# spark.kryoserializer.buffer.max                   512m
# spark.sql.execution.arrow.pyspark.enabled         true # For faster Pandas conversion
# spark.sql.adaptive.enabled                        true # Adaptive Query Execution (Spark 3.0+)
# spark.sql.adaptive.coalescePartitions.enabled     true
# spark.sql.adaptive.skewJoin.enabled               true

# Logging (default is INFO, can be set to WARN for less verbosity)
# log4j.rootCategory=WARN, console
# (This needs to be in a log4j.properties file, not spark-defaults.conf for Spark's own logging)

# Ensure these settings are compatible with your Spark version (bitnami/spark:3.3)
# The S3A settings are standard. Hive Metastore URI is also standard.
# The AWS SDK bundle required for s3a might already be included in bitnami/spark.
# If not, you'd need to add it via spark.jars.packages or by placing JARs in Spark's jars directory.
# Common packages: org.apache.hadoop:hadoop-aws, com.amazonaws:aws-java-sdk-bundle
# However, Bitnami Spark images usually come with Hadoop S3 support.
# Example: (Not needed if Bitnami image includes it)
# spark.jars.packages                               org.apache.hadoop:hadoop-aws:3.3.1,com.amazonaws:aws-java-sdk-bundle:1.11.901
# (Ensure versions match Spark's Hadoop version)
# Bitnami Spark 3.3 image likely uses Hadoop 3.3.x, so hadoop-aws:3.3.1 or similar is appropriate.
# For simplicity, assuming the Bitnami image has necessary S3A libs. The Spark service in docker-compose.yml
# also passes AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY as env vars, which s3a connector can pick up too.
# However, explicit config in spark-defaults.conf is cleaner.
# The env vars in spark master/worker are more for if applications running *inside* spark need to make direct AWS calls,
# not necessarily for Spark's own S3A filesystem configuration which prefers spark.hadoop properties.
# But s3a can fall back to env vars if Hadoop properties are not set.
# The current setup with spark.hadoop properties is the most robust.
# The AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY env vars in spark-master/worker in docker-compose.yml
# are redundant if these properties are set in spark-defaults.conf, but don't hurt.
# They are removed from Spark master/worker environment in docker-compose.yml to rely on this file.

# Final check on paths:
# SILVER_DATA_PATH = "s3a://silver/enriched_producer_data/"
# GOLD_FEATURES_PATH = "s3a://gold/proponente_features_parquet/"
# GOLD_CLICKHOUSE_TABLE_PATH = "s3a://gold/proponente_features_clickhouse/"
# Hive warehouse (optional): s3a://warehouse/spark-warehouse/
# All these paths will correctly use the S3A settings above.
# The 'warehouse' bucket is created by setup.sh. Other buckets (silver, gold) are also created.
# This file looks complete and correct for the environment.
