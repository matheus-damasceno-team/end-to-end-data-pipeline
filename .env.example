# .env.example file for Airflow configuration
#
# This file provides a template for the necessary environment variables.
# Copy this file to .env and fill in the appropriate values.
# Do NOT commit your actual .env file with sensitive credentials to version control.

# Set the UID for the Airflow user.
# On Linux/macOS, find this with `id -u` in your terminal.
# This ensures correct file permissions for mounted volumes (dags, logs, plugins).
# Example: AIRFLOW_UID=1000 (common default for first user on Linux)
# Example: AIRFLOW_UID=50000 (common on macOS, or use `id -u`)
AIRFLOW_UID=1000

# PostgreSQL Connection Details for Airflow
# These are used by Airflow to connect to its metadata database.
# The airflow-db service in docker-compose.yml will use these to initialize.
POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow_password_here # CHANGE THIS to a secure password in your .env file
POSTGRES_DB=airflow
POSTGRES_HOST=airflow-db # This should match the service name in docker-compose.yml
POSTGRES_PORT=5432       # Default PostgreSQL port

# Airflow Admin User (created by airflow-init service if they don't exist)
# These credentials will be used to log in to the Airflow Web UI.
_AIRFLOW_WWW_USER_USERNAME=airflow_admin # CHANGE THIS (e.g., admin)
_AIRFLOW_WWW_USER_PASSWORD=airflow_admin_password_here # CHANGE THIS to a secure password
_AIRFLOW_WWW_USER_FIRSTNAME=Airflow
_AIRFLOW_WWW_USER_LASTNAME=Admin
_AIRFLOW_WWW_USER_EMAIL=admin@example.com # CHANGE THIS

# Fernet key for encrypting connections and variables in Airflow.
# Generate one with: python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
# The official Airflow image will generate one if not set, but it's best practice to define it.
# If generated by the image, it will be written to airflow.cfg. If you restart without it,
# previously encrypted connections might become undecipherable.
# AIRFLOW__CORE__FERNET_KEY=your_fernet_key_here

# Airflow Executor
# LocalExecutor is good for simple, single-node setups.
# For more scalable setups, consider CeleryExecutor or KubernetesExecutor.
AIRFLOW__CORE__EXECUTOR=LocalExecutor

# --- Optional Airflow Configurations ---
# You can override many Airflow configurations by setting environment variables
# in the format AIRFLOW__SECTION__KEY=value
# Reference: https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html

# Whether to load example DAGs that come with Airflow
AIRFLOW__CORE__LOAD_EXAMPLES=False

# Enable XCom pickling (allows more complex data types to be passed via XComs)
AIRFLOW__CORE__ENABLE_XCOM_PICKLING=True

# Webserver settings
AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True # Exposes airflow.cfg in the UI (useful for debugging)

# Scheduler settings (defaults are usually fine for local development)
# AIRFLOW__SCHEDULER__DAG_DIR_LISTING_INTERVAL=60
# AIRFLOW__SCHEDULER__MIN_FILE_PROCESS_INTERVAL=60
# AIRFLOW__SCHEDULER__JOB_HEARTBEAT_SEC=15

# Logging settings
AIRFLOW__LOGGING__LOGGING_LEVEL=INFO
# To enable remote logging (e.g., to S3, GCS), you'd configure these:
# AIRFLOW__LOGGING__REMOTE_LOGGING=False
# AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER=s3a://your-s3-bucket/path/to/airflow-logs
# AIRFLOW__LOGGING__REMOTE_LOG_CONN_ID=your_aws_s3_connection_id # if using a connection

# --- Docker Compose Specific Variables (referenced in docker-compose.yml) ---
# These variables are used by docker-compose to construct the SQL_ALCHEMY_CONN
# and are also used by the airflow-db service.
# Ensure these match the PostgreSQL settings above.
AIRFLOW_DB_HOST=airflow-db      # Should match POSTGRES_HOST and the db service name
AIRFLOW_DB_PORT=5432          # Should match POSTGRES_PORT
AIRFLOW_DB_USER=airflow         # Should match POSTGRES_USER
AIRFLOW_DB_PASSWORD=airflow_password_here # Should match POSTGRES_PASSWORD
AIRFLOW_DB_NAME=airflow         # Should match POSTGRES_DB

# The SQL Alchemy connection string for Airflow to connect to its database.
# This is constructed using the variables above.
# Docker Compose will substitute these values when the services start.
# You generally do not need to change this line itself, but ensure the
# referenced variables (POSTGRES_USER, etc.) are set correctly.
AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}

# Default dags folder inside the Airflow containers
AIRFLOW__CORE__DAGS_FOLDER=/opt/airflow/dags
