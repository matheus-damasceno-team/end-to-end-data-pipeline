# Feast Feature Store Configuration
# This file defines the providers for the feature store.

project: agribusiness_credit_feast # Name of the Feast project, must match the directory name or the --project flag
registry: data/registry.db # Local registry for storing feature definitions (SQLite file). For MinIO: s3://your-feast-bucket/registry.db
provider: local # Using local provider for this setup. For cloud, this would be 'gcp' or 'aws'.
online_store:
  type: redis
  connection_string: "redis:6379" # Connects to the Redis service defined in docker-compose.yml
  # Example for Redis with password:
  # connection_string: "redis://:yourpassword@redis:6379"
offline_store:
  type: file # For demo purposes. This represents data in MinIO/S3.
  # In a production setup with Spark accessing MinIO, this would be configured differently,
  # often pointing to a data warehouse like BigQuery, Redshift, Snowflake, or data lakes (S3/GCS/ADLS via Spark/Trino).
  # Feast itself doesn't directly query 'file' type for offline joins in production.
  # The 'file' source in definitions.py will point to local paths (or S3 paths if using an S3-aware offline store).
  # For this project, Spark will read from MinIO (s3a:// paths) and could materialize features
  # that Feast then references. The 'file' type here is a simplification for local offline use.
  # To make Feast aware of data in MinIO for *materialization* or *offline retrieval* via its SDK,
  # you'd typically use a custom offline store or configure Spark as the query engine.
  # For this example, we'll assume feature data is pre-materialized to a location (e.g., Parquet files)
  # that Feast's FileSource can point to (conceptually, this would be on MinIO).

# Feature Serving API configuration (defaults are usually fine)
# serving:
#   type: http # Default, other option is grpc
#   host: 0.0.0.0
#   port: 6566 # As configured in docker-compose.yml for feast_serve

# If using S3 for the registry (recommended for non-local examples):
# registry:
#   registry_type: s3
#   path: s3://your-feast-registry-bucket/registry.db # Bucket must exist
#   s3_endpoint_override: http://minio:9000 # For MinIO

# To use MinIO as the offline store directly (more advanced, requires custom connectors or specific Feast versions/plugins):
# offline_store:
#   type: custom_offline_store_for_s3_or_similar
#   # Or rely on Spark/Trino to access data in MinIO, and Feast points to those tables.

# For this setup, the `feast_serve` service in docker-compose.yml will use this file.
# The `FEAST_S3_ENDPOINT_URL` env var is set for the feast_serve service
# if the registry or sources were S3-based (e.g. s3://... for registry path or FileSource path).
# Since we use a local file registry here (`data/registry.db`), it will be created inside the feast_serve container.
# For persistence of the registry across container restarts, you might want to volume mount the `data` directory
# or switch to an S3-backed registry using MinIO.
# e.g., in feast_serve service in docker-compose.yml:
# volumes:
#   - ./feast_repo:/app/feast_repo
#   - feast_registry_data:/app/feast_repo/data # New volume for registry persistence

# The `project` name here ('agribusiness_credit_feast') should match the directory name
# if you run `feast apply` from the parent of `agribusiness_credit_feast/`.
# Or, if you run `feast apply` from within `feast_repo/`, Feast uses the current directory context.
# The `feast_serve` command in docker-compose specifies `--project feast_repo` which means
# it expects `feature_store.yaml` inside a directory named `feast_repo`.
# Our mounted volume is `./feast_repo:/app/feast_repo` and working_dir is `/app/feast_repo`.
# The `FEAST_PROJECT: feast_repo` env var in docker-compose also helps Feast identify the project context.
# The `FEAST_FS_YAML_FILE` env var explicitly points to this file.

# To initialize or update the feature store with definitions from definitions.py:
# Run `feast apply` from the `feast_repo` directory (or the directory specified as `FEAST_PROJECT`).
# This will be done in the setup.sh script.
project_id_override: "agribusiness_credit_feast" # Explicitly set project ID
entity_key_serialization_version: 2 # Recommended for newer Feast versions
project_metadata:
    team: "Credit Analysis Team"
    version: "0.1.0"
    description: "Feature store for agribusiness credit risk assessment"
flags:
    # Example flags, check Feast documentation for current options
    # on_demand_transforms: True # Enable on-demand transforms if you use them
    # prophet_offline_store_batch_size: 1000 # Example of a specific store configuration
    # By default, Feast uses a SQLite registry file (registry.db) in a data/ subdirectory of your feature repository.
    # To make the registry persistent and accessible by other tools (like a Spark job that needs to read feature defs),
    # consider using an S3-backed registry with MinIO:
    # registry: s3://feast-registry/registry.db # Ensure this bucket exists in MinIO
    # And configure AWS_S3_ENDPOINT_URL for feast CLI and server.
    # For simplicity in this example, we stick to the local SQLite registry.
    # The `setup.sh` script will run `feast apply` which will create `data/registry.db` inside the container.
    # If you want it to persist, add a volume to docker-compose.yml for `/app/feast_repo/data`
    # in the `feast_serve` service.
    # Example:
    # volumes:
    #   - ./feast_repo:/app/feast_repo
    #   - feast_registry_data:/app/feast_repo/data
    # And define `feast_registry_data:` in top-level volumes.
    # For now, we'll let it be ephemeral within the container for simplicity of setup.
    # If `feast apply` is run by `setup.sh` against the `feast_serve` container, this path will be relative to `/app/feast_repo/`.
    # If run locally, it's relative to the CWD where `feast_repo` is.
    # The `feast_serve` command uses `FEAST_FS_YAML_FILE: /app/feast_repo/feature_store.yaml`.
    # The `feast apply` in `setup.sh` will likely be `docker-compose exec feast_serve feast apply`.
    # This means it runs within the container's context, using `/app/feast_repo` as the project root.
    # So `data/registry.db` will be created as `/app/feast_repo/data/registry.db`.
    # If you want the registry to be in MinIO, you would change the `registry` path to an S3 path
    # and ensure MinIO credentials and endpoint are available to `feast apply` and `feast serve`.
    # For this example, we'll keep the local SQLite registry.
    # The `project` field value will be used as the feature repo name.
    # It's crucial that this matches how you refer to the project when running Feast CLI commands.
    # The `feast_serve` in `docker-compose.yml` uses `FEAST_PROJECT: feast_repo`,
    # and `command: feast serve --project feast_repo`.
    # Let's ensure `project: feast_repo` for consistency.
project: feast_repo # Overriding the previous project name to match docker-compose context
offline_store_skip_materialization: false # Default is false, set to true if you don't want Feast to try materializing
# For S3 backed registry with MinIO:
# registry:
#  registry_type: s3
#  path: "s3://feast-registry-bucket/registry.db" # Make sure this bucket exists
#  s3_endpoint_override: "http://minio:9000"

# For this example, we'll stick to a local registry for simplicity.
# The registry will be created inside the feast_serve container at /app/feast_repo/data/registry.db
# To make it persistent, you'd add a volume to the feast_serve service in docker-compose.yml:
# volumes:
#   - ./feast_repo:/app/feast_repo
#   - feast_registry_volume:/app/feast_repo/data
# ...and define `feast_registry_volume` at the top level.
# For now, it will be ephemeral.
# The `feast apply` in setup.sh will create it.
online_store_type: redis # Explicitly stating, though type under online_store is preferred
offline_store_type: file # Explicitly stating, though type under offline_store is preferred
feature_server_image: "" # Not needed as we are running feast serve directly
python_server_image: "" # Not needed
python_server_port: 6566 # Matches docker-compose
python_server_host: "0.0.0.0" # Matches docker-compose
enable_auth: false # No auth for local dev
go_feature_server: false # Using Python server
# Ensure `project` field is consistent. `feast_repo` is good as it matches the directory name.
# The `feast_serve` command in docker-compose also refers to `feast_repo`.
project: feast_repo # Final confirmation
registry: data/registry.db # Path for the local SQLite registry file
provider: local
online_store:
    type: redis
    connection_string: "redis://redis:6379/0" # Added /0 for default Redis DB
offline_store: # This is where features are read from for training, batch scoring, or loading into the online store
    type: file # Indicates that the source for feature views will be local files (e.g., Parquet)
    # In a real setup, this would be a data lake (S3, GCS, ADLS) or warehouse (BigQuery, Snowflake, Redshift)
    # and paths in FileSource would be s3a://... etc.
    # For this example, 'file' type means Feast expects local file paths in feature view sources.
    # These files would be the output of the Spark job, conceptually stored in MinIO and made available
    # to the Feast container (e.g. by mounting or using an S3-compatible FileSource if supported directly).
    # Let's assume for now that `FileSource.path` will point to a path accessible by the Feast service.
    # If Spark writes to MinIO, and Feast needs to read that for `feast materialize`,
    # then Feast's offline store or the `FileSource` itself needs S3 capabilities.
    # The `FileSource` in Feast can take s3:// paths if Hadoop/S3 libs are present.
    # The `feast-python-server` image might have these.
    # Let's plan for `FileSource` paths to be `s3a://` pointing to MinIO.
    # This means the `feast_serve` container (and `feast apply` executions)
    # will need MinIO access (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, FEAST_S3_ENDPOINT_URL).
    # These are already set in the docker-compose for `feast_serve`.

# Feature serving (online GET) and retrieval (historical GET) settings
# Defaults are generally fine for local setup
# telemetry: false # Disable telemetry for local dev
# repo_path: . # Path to the feast repository (feature_store.yaml, definitions)
# Default is the current directory when running `feast` commands.
# The `feast_serve` in docker-compose sets working_dir: /app/feast_repo
# and mounts `./feast_repo` to `/app/feast_repo`.
# So, `registry: data/registry.db` will be `/app/feast_repo/data/registry.db`.
# `definitions.py` will be found in `/app/feast_repo/definitions.py`.
# This setup is correct.
project: feast_repo
registry: data/registry.db # Local SQLite registry. Will be created inside the container.
provider: local
online_store:
  type: redis
  connection_string: "redis://redis:6379/0"
offline_store:
  type: file # Signifies that feature view sources are files (e.g., Parquet)
  # When using s3a:// paths in FileSource, ensure Feast has S3 access configured.
  # (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, FEAST_S3_ENDPOINT_URL env vars in feast_serve service)
flags:
  # Keep this simple for now. Add flags if specific behavior is needed.
  # on_demand_transforms: true # if you plan to use on_demand_feature_views
  # entity_key_serialization_version: 2 # For compatibility with newer SDKs if issues arise
  pass_through_errors_from_serving: true # Useful for debugging
  # By default, the registry is stored in a local SQLite file named registry.db in a subdirectory called data within your feature repository.
  # This means it will be at /app/feast_repo/data/registry.db in the container.
  # This is fine for local development. For more robust setups, use an S3-backed registry (MinIO).
  # Example for MinIO registry (ensure bucket exists):
  # registry:
  #   registry_type: s3
  #   path: s3://feast-registry/agribusiness/registry.db
  #   s3_endpoint_override: http://minio:9000
  # The environment variables for S3 access are already in the feast_serve service in docker-compose.yml.
  # For now, we'll stick to the local SQLite registry.
entity_key_serialization_version: 2 # Good practice for recent Feast versions.
python_config: # For any Python specific configurations, if necessary
    # Example: custom_feast_module: "my_custom_module"
    pass_through_errors: true # Same as the flag above
# No explicit feature_server or serving block needed if using default Python server with above settings.
# The feast_serve command in docker-compose handles host/port.
# The project name is critical. It should match the directory name where feature_store.yaml and definitions live.
# Our directory is `feast_repo`, so `project: feast_repo` is correct.
# `feast apply` will be run from within this directory context in the container.
# `feast materialize` would also use this context.
# The `offline_store.type: file` means that for `FileSource` in `definitions.py`,
# the `path` attribute will be treated as a file path. If this path is an `s3a://` URI,
# Feast (if built with Hadoop S3 libs) should be able to access it using the S3 env vars.
# The standard Feast images usually include these.
