# Trino Hive Connector Configuration
# This catalog will be named 'hive' in Trino (matches filename)

connector.name=hive
hive.metastore.uri=thrift://hive-metastore:9083

# Configuration for accessing data in S3 (MinIO) via Hive
# These are needed if Hive tables are stored on S3/MinIO.
hive.s3.endpoint=http://minio:9000
hive.s3.aws-access-key=admin
hive.s3.aws-secret-key=password
hive.s3.path-style-access=true # Required for MinIO

# Optional: S3 SSL configuration (if MinIO uses HTTPS)
# hive.s3.ssl.enabled=false

# Optional: Performance tuning for S3 access
# hive.s3.max-connections=500
# hive.s3.multipart.min-file-size=128MB
# hive.s3.multipart.min-part-size=16MB

# Schema/Database mapping (optional)
# hive.database-name-for-schema=default # Maps Trino schemas to Hive databases

# Caching (optional, for frequently accessed metadata or data)
# hive.metastore-cache-ttl=1h
# hive.file-status-cache-ttl=1h
# hive.partition-cache-ttl=1h

# Security (e.g., impersonation or passthrough)
# hive.security=NONE # Default
# hive.s3.iam-role= # If using IAM roles with AWS S3 (not MinIO typically)

# File formats and SerDes
# Trino Hive connector supports Parquet, ORC, Avro, CSV, JSON, RCFile, SequenceFile by default.
# Ensure your Hive tables use supported formats. Parquet is recommended.

# For tables created by Spark and registered in Hive Metastore,
# Trino should be able to query them if they point to S3/MinIO paths
# and use standard formats like Parquet.
# The spark-defaults.conf ensures Spark uses s3a:// paths.
# Hive Metastore stores these s3a:// paths as table locations.
# Trino's Hive connector, with these S3 settings, can then read from those locations.
# Ensure the Hive version used by Metastore and Spark is compatible with Trino's Hive connector.
# Trino 426 should be compatible with Hive 3.x.
