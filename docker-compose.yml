networks:
  data_pipeline_net:
    driver: bridge

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.6.1
    ## CORREÇÃO: Adicionada plataforma para garantir consistência e performance.
    platform: linux/amd64
    container_name: zookeeper
    networks:
      - data_pipeline_net
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka:
    image: confluentinc/cp-kafka:7.6.1
    ## CORREÇÃO: Adicionada plataforma para garantir consistência e performance.
    platform: linux/amd64
    container_name: kafka
    networks:
      - data_pipeline_net
    ports:
      - "9092:9092"
      - "29092:29092"
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_CONFLUENT_LICENSE_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_CONFLUENT_BALANCER_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1

  kafka-ui:
    container_name: kafka-ui
    image: provectuslabs/kafka-ui:latest
    ## CORREÇÃO: Adicionada plataforma para garantir consistência e performance.
    platform: linux/amd64
    networks:
      - data_pipeline_net
    ports:
      - "8085:8080"
    environment:
      DYNAMIC_CONFIG_ENABLED: 'true'
    volumes:
      - ./config/kafka-ui/config.yml:/etc/kafkaui/dynamic_config.yaml

  minio:
    image: minio/minio:latest
    ## CORREÇÃO: Adicionada plataforma para garantir consistência e performance.
    platform: linux/amd64
    container_name: minio
    networks:
      - data_pipeline_net
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio_data:/data
    environment:
      MINIO_ROOT_USER: admin
      MINIO_ROOT_PASSWORD: password
    command: server /data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3

  mc:
    image: minio/mc:latest
    platform: linux/amd64
    container_name: mc
    networks:
      - data_pipeline_net
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: /bin/sh
    command:
      - -c
      - |
        set -e
        echo '--- INÍCIO DO SCRIPT MC (Versão Final Simplificada) ---'

        echo '[1/2] Configurando alias para o MinIO...'
        until /usr/bin/mc alias set local http://minio:9000 admin password; do
          echo '...MinIO não está pronto, aguardando 1s...';
          sleep 1;
        done
        echo 'Alias do MinIO configurado.'

        echo '[2/2] Criando buckets necessários...'
        /usr/bin/mc mb local/bronze --ignore-existing
        /usr/bin/mc mb local/silver --ignore-existing
        /usr/bin/mc mb local/gold --ignore-existing
        /usr/bin/mc mb local/warehouse --ignore-existing
        echo 'Buckets criados. Usando permissões padrão do usuário root.'

        echo '--- SCRIPT MC FINALIZADO COM SUCESSO ---'
    restart: "no"

  trino:
    build:
      context: ./services/trino
    container_name: trino
    platform: linux/amd64
    hostname: trino
    networks:
      - data_pipeline_net
    ports:
      - "8080:8080"
    depends_on:
      - metastore
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/ui/"]
      interval: 10s
      timeout: 5s
      retries: 5
    extra_hosts:
      - "host.docker.internal:host-gateway"
      - "trino:127.0.0.1"

  metastore-db:
    image: mariadb:10.5
    ## CORREÇÃO: Adicionada plataforma para garantir consistência e performance.
    platform: linux/amd64
    container_name: metastore-db
    networks:
      - data_pipeline_net
    volumes:
      - metastore-db:/var/lib/mysql
    environment:
      # A senha é definida aqui para o MariaDB usar na inicialização
      - MYSQL_ROOT_PASSWORD=password
    healthcheck:
      # E a mesma senha é usada diretamente no comando abaixo.
      # A sintaxe correta é -pSENHA (tudo junto, sem espaço).
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost", "-u", "root", "-ppassword"]
      interval: 10s
      timeout: 5s
      retries: 5
    extra_hosts:
      - "host.docker.internal:host-gateway"

  create-metastore-schema:
    build:
      context: ./services/hive_metastore
      dockerfile: Dockerfile.schema
    platform: linux/amd64
    container_name: create-metastore-schema
    depends_on:
      metastore-db:
        condition: service_healthy
    networks:
      - data_pipeline_net
    entrypoint: /bin/bash
    command: ["/opt/hive/bin/init-schema.sh"]
    environment:
      - DB_DRIVER=mysql
    restart: "no"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - ./metastore/core-site.xml:/opt/hive/conf/core-site.xml
      - ./metastore/hive-site.xml:/opt/hive/conf/hive-site.xml

  metastore:
    build:
      context: ./services/hive_metastore
    platform: linux/amd64
    container_name: metastore
    networks:
      - data_pipeline_net
    ports:
      - "9083:9083"
    environment:
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=password
      - AWS_REGION=us-east-1
      - AWS_DEFAULT_REGION=us-east-1
      - SERVICE_NAME=metastore
      - DB_DRIVER=mysql
      - IS_RESUME=true
      - SERVICE_OPTS=-Djavax.jdo.option.ConnectionDriverName=com.mysql.cj.jdbc.Driver -Djavax.jdo.option.ConnectionURL=jdbc:mysql://metastore-db:3306/metastore_db -Djavax.jdo.option.ConnectionUserName=root -Djavax.jdo.option.ConnectionPassword=password -Dfs.s3a.endpoint=http://minio:9000 -Dfs.s3a.access.key=admin -Dfs.s3a.secret.key=password -Dfs.s3a.path.style.access=true -Dhive.metastore.warehouse.dir=s3a://warehouse/ -Dfs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider -Dfs.s3a.region=us-east-1 -Dfs.s3a.bucket.all.region.lookup.disable=true -Dfs.s3a.signer.override=S3SignerType -Dfs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem -Daws.region=us-east-1
    depends_on:
      metastore-db:
        condition: service_healthy
      create-metastore-schema:
        condition: service_completed_successfully
      mc:
        condition: service_completed_successfully
    restart: always
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "9083"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - ./metastore/core-site.xml:/opt/hive/conf/core-site.xml
      - ./metastore/hive-site.xml:/opt/hive/conf/hive-site.xml

  spark-master:
    image: bitnami/spark:3.5
    container_name: spark-master
    platform: linux/amd64
    networks:
      - data_pipeline_net
    ports:
      - "8081:8080"
      - "7077:7077"
    volumes:
      - ./services/spark_jobs:/opt/bitnami/spark/jobs
      #- ./spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
      - ./metastore/core-site.xml:/opt/bitnami/spark/conf/core-site.xml
      - ./metastore/hive-site.xml:/opt/bitnami/spark/conf/hive-site.xml
    environment:
      SPARK_MODE: master
      SPARK_RPC_AUTHENTICATION_ENABLED: "no"
      SPARK_RPC_ENCRYPTION_ENABLED: "no"
      SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED: "no"
      SPARK_SSL_ENABLED: "no"
      # Add Hive configuration
      HIVE_METASTORE_URIS: "thrift://metastore:9083"
    depends_on:
     - metastore

  spark-worker:
    image: bitnami/spark:3.5
    ## CORREÇÃO: Adicionada plataforma para garantir consistência e performance.
    platform: linux/amd64
    container_name: spark-worker
    networks:
      - data_pipeline_net
    ports:
      - "8082:8081"
    depends_on:
      - spark-master
    volumes:
      - ./services/spark_jobs:/opt/bitnami/spark/jobs
      - ./spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_WORKER_MEMORY: 2G
      SPARK_WORKER_CORES: 2
      SPARK_RPC_AUTHENTICATION_ENABLED: "no"
      SPARK_RPC_ENCRYPTION_ENABLED: "no"
      SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED: "no"
      SPARK_SSL_ENABLED: "no"

  # ingestion-consumer:
  #   image: bitnami/spark:3.5
  #   platform: linux/amd64
  #   container_name: ingestion-consumer
  #   networks:
  #     - data_pipeline_net
  #   depends_on:
  #     kafka:
  #       condition: service_started
  #     metastore:
  #       condition: service_healthy
  #     spark-master:
  #       condition: service_started
  #     mc:
  #       condition: service_completed_successfully
  #   volumes:
  #     - ./services/spark_jobs:/opt/bitnami/spark/jobs
  #     - ./services/producer_simulator/producer_schema.avsc:/opt/bitnami/spark/jobs/producer_schema.avsc:ro
  #     - ./spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
  #     - ./metastore/core-site.xml:/opt/bitnami/spark/conf/core-site.xml
  #     - ./metastore/hive-site.xml:/opt/bitnami/spark/conf/hive-site.xml
  #   environment:
  #     KAFKA_BROKER: kafka:9092
  #     KAFKA_TOPIC: dados_produtores
  #     ICEBERG_WAREHOUSE: s3a://warehouse
  #     ICEBERG_TABLE_NAME: bronze.dados_produtores_agro
  #     AWS_ACCESS_KEY_ID: admin
  #     AWS_SECRET_ACCESS_KEY: password
  #     AWS_S3_ENDPOINT_URL: http://minio:9000
  #     AWS_REGION: us-east-1
  #     AWS_DEFAULT_REGION: us-east-1
  #   command:
  #     - /bin/bash
  #     - -c
  #     - |
  #       echo "Setting up AWS environment variables..."
  #       export AWS_REGION=us-east-1
  #       export AWS_DEFAULT_REGION=us-east-1
  #       export AWS_ACCESS_KEY_ID=admin
  #       export AWS_SECRET_ACCESS_KEY=password
        
  #       echo "Waiting for all dependencies to be ready..."
  #       sleep 30
        
  #       echo "Starting Spark job with Iceberg..."
  #       /opt/bitnami/spark/bin/spark-submit \
  #         --master spark://spark-master:7077 \
  #         --packages org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2,org.apache.iceberg:iceberg-aws-bundle:1.5.2,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.spark:spark-avro_2.12:3.5.0 \
  #         --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \
  #         --conf spark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkCatalog \
  #         --conf spark.sql.catalog.spark_catalog.type=hive \
  #         --conf spark.sql.catalog.spark_catalog.uri=thrift://metastore:9083 \
  #         --conf spark.sql.catalog.spark_catalog.warehouse=s3a://warehouse \
  #         --conf spark.sql.catalog.spark_catalog.io-impl=org.apache.iceberg.aws.s3.S3FileIO \
  #         --conf spark.sql.catalog.spark_catalog.s3.endpoint=http://minio:9000 \
  #         --conf spark.sql.catalog.spark_catalog.s3.path-style-access=true \
  #         --conf spark.sql.catalog.spark_catalog.s3.access-key-id=admin \
  #         --conf spark.sql.catalog.spark_catalog.s3.secret-access-key=password \
  #         --conf spark.sql.catalog.spark_catalog.s3.region=us-east-1 \
  #         --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000 \
  #         --conf spark.hadoop.fs.s3a.access.key=admin \
  #         --conf spark.hadoop.fs.s3a.secret.key=password \
  #         --conf spark.hadoop.fs.s3a.path.style.access=true \
  #         --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider \
  #         --conf spark.hadoop.fs.s3a.region=us-east-1 \
  #         --conf spark.hadoop.fs.s3a.bucket.all.region.lookup.disable=true \
  #         --conf spark.hadoop.fs.s3a.signer.override=S3SignerType \
  #         /opt/bitnami/spark/jobs/ingestion_to_iceberg.py

  ingestion-consumer:
    image: bitnami/spark:3.5
    platform: linux/amd64
    container_name: ingestion-consumer
    networks:
      - data_pipeline_net
    depends_on:
      kafka:
        condition: service_started
      metastore:
        condition: service_healthy
      spark-master:
        condition: service_started
      mc:
        condition: service_completed_successfully
    volumes:
      - ./services/spark_jobs:/opt/bitnami/spark/jobs
      - ./services/producer_simulator/producer_schema.avsc:/opt/bitnami/spark/jobs/producer_schema.avsc:ro
      - ./spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
      - ./metastore/core-site.xml:/opt/bitnami/spark/conf/core-site.xml
      - ./metastore/hive-site.xml:/opt/bitnami/spark/conf/hive-site.xml
    environment:
      KAFKA_BROKER: kafka:9092
      KAFKA_TOPIC: dados_produtores
      ICEBERG_WAREHOUSE: s3a://warehouse
      ICEBERG_TABLE_NAME: bronze.dados_produtores_agro
      AWS_ACCESS_KEY_ID: admin
      AWS_SECRET_ACCESS_KEY: password
      AWS_S3_ENDPOINT_URL: http://minio:9000
      AWS_REGION: us-east-1
      AWS_DEFAULT_REGION: us-east-1
      PYTHONPATH: /opt/bitnami/spark/python:/opt/bitnami/spark/python/lib/pyspark.zip:/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip
    command:
      - /bin/bash
      - -c
      - |
        echo "Setting up environment..."
        export AWS_REGION=us-east-1
        export AWS_DEFAULT_REGION=us-east-1
        export AWS_ACCESS_KEY_ID=admin
        export AWS_SECRET_ACCESS_KEY=password
        
        echo "Ensuring file permissions..."
        chmod +x /opt/bitnami/spark/jobs/ingestion_to_iceberg.py
        
        echo "Waiting for dependencies..."
        sleep 30
        
        echo "Starting Spark job with Iceberg..."
        cd /opt/bitnami/spark/jobs
        
        /opt/bitnami/spark/bin/spark-submit \
          --master spark://spark-master:7077 \
          --deploy-mode client \
          --packages org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.3,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.spark:spark-avro_2.12:3.5.0 \
          --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \
          --conf spark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkCatalog \
          --conf spark.sql.catalog.spark_catalog.type=hive \
          --conf spark.sql.catalog.spark_catalog.uri=thrift://metastore:9083 \
          --conf spark.sql.catalog.spark_catalog.warehouse=s3a://warehouse \
          --conf spark.sql.catalog.spark_catalog.io-impl=org.apache.iceberg.hadoop.HadoopFileIO \
          --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000 \
          --conf spark.hadoop.fs.s3a.access.key=admin \
          --conf spark.hadoop.fs.s3a.secret.key=password \
          --conf spark.hadoop.fs.s3a.path.style.access=true \
          --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider \
          --conf spark.hadoop.fs.s3a.region=us-east-1 \
          --conf spark.hadoop.fs.s3a.bucket.all.region.lookup.disable=true \
          --conf spark.hadoop.fs.s3a.signer.override=S3SignerType \
          --conf spark.hadoop.fs.s3a.connection.ssl.enabled=false \
          --conf spark.hadoop.fs.s3a.fast.upload=true \
          --conf spark.hadoop.fs.s3a.multipart.size=67108864 \
          --conf spark.hadoop.fs.s3a.fast.upload.buffer=bytebuffer \
          --conf spark.driver.extraJavaOptions="-Daws.region=us-east-1" \
          --conf spark.executor.extraJavaOptions="-Daws.region=us-east-1" \
          ./ingestion_to_iceberg.py

  kafka-cleaner:
    image: confluentinc/cp-kafka:7.6.1
    ## CORREÇÃO: Adicionada plataforma para garantir consistência e performance.
    platform: linux/amd64
    container_name: kafka-cleaner
    networks:
      - data_pipeline_net
    depends_on:
      - kafka
    command: >
      bash -c "
        echo 'Waiting for Kafka to be ready...' &&
        sleep 15 &&
        echo 'Deleting Kafka topic: dados_produtores...' &&
        kafka-topics --bootstrap-server kafka:9092 --delete --topic dados_produtores --if-exists &&
        echo 'Topic deleted. Creating a new one...' &&
        kafka-topics --bootstrap-server kafka:9092 --create --topic dados_produtores --partitions 3 --replication-factor 1 &&
        echo 'Topic created successfully.'
      "
    restart: on-failure

  producer-simulator:
    build:
      context: ./services/producer_simulator
    ## CORREÇÃO: Adicionada plataforma para garantir consistência e performance.
    platform: linux/amd64
    container_name: producer-simulator
    networks:
      - data_pipeline_net
    depends_on:
      - kafka
    environment:
      KAFKA_BROKER: kafka:9092
      KAFKA_TOPIC: dados_produtores
      
  # SERVIÇOS COMENTADOS PRESERVADOS ABAIXO

  # clickhouse:
  #   image: clickhouse/clickhouse-server:24.5
  #   ## CORREÇÃO: Adicionada plataforma para garantir consistência e performance.
  #   platform: linux/amd64
  #   container_name: clickhouse
  #   networks:
  #     - data_pipeline_net
  #   ports:
  #     - "8123:8123"
  #     - "9004:9000"
  #   volumes:
  #     - clickhouse_data:/var/lib/clickhouse
  #     - ./clickhouse_config/config.xml:/etc/clickhouse-server/config.xml
  #     - ./clickhouse_config/users.xml:/etc/clickhouse-server/users.xml
  #   ulimits:
  #     nofile:
  #       soft: 262144
  #       hard: 262144

  # redis:
  #   image: redis:7-alpine
  #   ## CORREÇÃO: Adicionada plataforma para garantir consistência e performance.
  #   platform: linux/amd64
  #   container_name: redis
  #   networks:
  #     - data_pipeline_net
  #   ports:
  #     - "6379:6379"
  #   volumes:
  #     - redis_data:/data
  #   healthcheck:
  #     test: ["CMD", "redis-cli", "ping"]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 5

  # feast-serve:
  #   image: python:3.9-slim
  #   ## CORREÇÃO: Adicionada plataforma para garantir consistência e performance.
  #   platform: linux/amd64
  #   container_name: feast-serve
  #   restart: unless-stopped
  #   networks:
  #     - data_pipeline_net
  #   ports:
  #     - "6565:6565"
  #     - "6566:6566"
  #   volumes:
  #     - ./feast_repo:/app/feast_repo
  #   environment:
  #     AWS_ACCESS_KEY_ID: admin
  #     AWS_SECRET_ACCESS_KEY: password
  #     AWS_S3_ENDPOINT_URL: http://minio:9000
  #     FEAST_USAGE: "False"
  #   working_dir: /app/feast_repo
  #   command: >
  #     bash -c "
  #       echo 'Installing system dependencies...' &&
  #       apt-get update && apt-get install -y --no-install-recommends 
  #         gcc g++ curl netcat-openbsd && 
  #       rm -rf /var/lib/apt/lists/* &&
  #       echo 'Installing Feast and dependencies...' &&
  #       pip install --no-cache-dir feast[aws]==0.37.1 boto3 &&
  #       echo 'Feast version:' && feast version &&
  #       echo 'Starting Feast server...' &&
  #       feast serve --host 0.0.0.0 --port 6566
  #     "
  #   depends_on:
  #     redis:
  #       condition: service_healthy
  #     minio:
  #       condition: service_healthy
  #   healthcheck:
  #     test: ["CMD-SHELL", "nc -z localhost 6566 || exit 1"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 5
  #     start_period: 60s

  # jupyter-lab:
  #   image: jupyter/all-spark-notebook:latest
  #   ## CORREÇÃO: Adicionada plataforma para garantir consistência e performance.
  #   platform: linux/amd64
  #   container_name: jupyter-lab
  #   user: root
  #   networks:
  #     - data_pipeline_net
  #   ports:
  #     - "8888:8888"
  #   volumes:
  #     - ./notebooks:/home/jovyan/work
  #     - ./spark-defaults.conf:/usr/local/spark/conf/spark-defaults.conf:ro
  #   environment:
  #     JUPYTER_ENABLE_LAB: "yes"
  #     SPARK_MASTER: "spark://spark-master:7077"
  #     AWS_ACCESS_KEY_ID: admin
  #     AWS_SECRET_ACCESS_KEY: password
  #     AWS_S3_ENDPOINT_URL: "http://minio:9000"
  #     JUPYTER_TOKEN: ""
  #     NB_UID: 1000
  #     NB_GID: 100
  #     CHOWN_HOME: "yes"
  #     CHOWN_HOME_OPTS: "-R"
  #   depends_on:
  #     - spark-master
  #     - minio
  #     - feast-serve
  #     - trino
  #   command: >
  #     bash -c "
  #       echo 'Installing additional packages...' &&
  #       pip install --no-cache-dir 
  #         feast[aws]==0.37.1 
  #         boto3 
  #         s3fs 
  #         clickhouse-connect 
  #         trino 
  #         dbt-core 
  #         dbt-clickhouse 
  #         plotly 
  #         seaborn &&
  #       echo 'Fixing permissions...' &&
  #       fix-permissions /home/jovyan &&
  #       echo 'Starting Jupyter Lab...' &&
  #       start-notebook.sh --NotebookApp.token='' --NotebookApp.password='' --ServerApp.disable_check_xsrf=True
  #     "

  # superset:
  #   image: apache/superset:3.1.1
  #   ## CORREÇÃO: Adicionada plataforma para garantir consistência e performance.
  #   platform: linux/amd64
  #   container_name: superset
  #   networks:
  #     - data_pipeline_net
  #   ports:
  #     - "8089:8088"
  #   depends_on:
  #     - clickhouse
  #     - trino
  #   environment:
  #     ADMIN_USERNAME: admin
  #     ADMIN_EMAIL: admin@superset.com
  #     ADMIN_PASSWORD: admin
  #     SUPERSET_SECRET_KEY: "a_very_secret_key_for_superset"
  #     DB_HOST: superset-db
  #     DB_PORT: 5432
  #     DB_USER: superset
  #     DB_PASS: superset
  #     DB_NAME: superset
  #     PYTHONPATH: "/app/pythonpath"
  #     SUPERSET_LOAD_EXAMPLES: "no"
  #   volumes:
  #     - superset_data:/app/superset_home
  #     - ./superset_config.py:/app/superset_config.py
  #   command: >
  #     bash -c "superset db upgrade && \
  #               superset init && \
  #               gunicorn --bind 0.0.0.0:8088 --workers 2 --timeout 60 'superset.app:create_app()'"

  # dbt:
  #   image: python:3.9-slim
  #   ## CORREÇÃO: Adicionada plataforma para garantir consistência e performance.
  #   platform: linux/amd64
  #   container_name: dbt-clickhouse
  #   networks:
  #     - data_pipeline_net
  #   volumes:
  #     - ./dbt_project:/usr/app/dbt_project
  #     - ./profiles.yml:/root/.dbt/profiles.yml
  #   working_dir: /usr/app/dbt_project
  #   depends_on:
  #     - clickhouse
  #   command: >
  #     bash -c "
  #       echo 'Installing system dependencies...' &&
  #       apt-get update && apt-get install -y --no-install-recommends 
  #         gcc g++ curl git && 
  #       rm -rf /var/lib/apt/lists/* &&
  #       echo 'Installing dbt-clickhouse and dependencies...' &&
  #       pip install --no-cache-dir dbt-core==1.8.2 dbt-clickhouse==1.8.2 &&
  #       echo 'dbt version:' && dbt --version &&
  #       echo 'dbt-clickhouse installed successfully. Container ready for exec commands.' &&
  #       tail -f /dev/null
  #     "

  airflow-db:
    image: postgres:13
    container_name: airflow-db
    platform: linux/amd64
    networks:
      - data_pipeline_net
    environment:
      - POSTGRES_USER=${POSTGRES_USER:-airflow}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-airflow}
      - POSTGRES_DB=${POSTGRES_DB:-airflow}
    volumes:
      - airflow_db_data:/var/lib/postgresql/data # Named volume defined below
    ports:
      - "5433:5432" # Host:Container
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $${POSTGRES_USER:-airflow} -d $${POSTGRES_DB:-airflow} -q"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  airflow-init:
    image: apache/airflow:2.8.1
    container_name: airflow-init
    platform: linux/amd64
    depends_on:
      airflow-db:
        condition: service_healthy
    networks:
      - data_pipeline_net
    env_file:
      - .env
    environment:
      - AIRFLOW_UID=${AIRFLOW_UID:-1000}
      # Ensure SQL_ALCHEMY_CONN is correctly interpreted by entrypoint
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER:-airflow}:${POSTGRES_PASSWORD:-airflow}@airflow-db:${POSTGRES_PORT:-5432}/${POSTGRES_DB:-airflow}
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      # - ./airflow/config:/opt/airflow/config # Optional: if you have a custom airflow.cfg
    # Not overriding entrypoint, so it uses the image's default entrypoint
    # which handles user setup based on AIRFLOW_UID.
    command:
      - bash
      - -c
      - |
        set -e
        echo "Airflow initialization: Waiting for database..."
        # The default entrypoint should have set up the user.
        # We still need to wait for DB before running airflow commands.
        timeout_duration=60 # seconds
        start_time=$$(date +%s)
        until nc -z "$${POSTGRES_HOST:-airflow-db}" "$${POSTGRES_PORT:-5432}"; do
          current_time=$$(date +%s)
          elapsed_time=$((current_time - start_time))
          if [ "$$elapsed_time" -ge "$$timeout_duration" ]; then
            echo "Timeout waiting for PostgreSQL. Exiting."
            exit 1
          fi
          echo "PostgreSQL is unavailable - sleeping for 1 second..."
          sleep 1
        done
        echo "PostgreSQL is up."

        echo "Running 'airflow db upgrade'..."
        airflow db upgrade

        echo "Creating Airflow admin user: $${_AIRFLOW_WWW_USER_USERNAME:-airflow}"
        airflow users create \
          --username "$${_AIRFLOW_WWW_USER_USERNAME:-airflow}" \
          --password "$${_AIRFLOW_WWW_USER_PASSWORD:-airflow}" \
          --firstname "$${_AIRFLOW_WWW_USER_FIRSTNAME:-Airflow}" \
          --lastname "$${_AIRFLOW_WWW_USER_LASTNAME:-Admin}" \
          --email "$${_AIRFLOW_WWW_USER_EMAIL:-admin@example.com}" \
          --role Admin || echo "Admin user already exists or command failed (this might be okay if user exists)."

        echo "Airflow initialization process complete. Container will exit."
    restart: "no"

  airflow-webserver:
    image: apache/airflow:2.8.1
    container_name: airflow-webserver
    platform: linux/amd64
    restart: unless-stopped
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    networks:
      - data_pipeline_net
    env_file:
      - .env
    environment:
      - AIRFLOW_UID=${AIRFLOW_UID:-1000}
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER:-airflow}:${POSTGRES_PASSWORD:-airflow}@airflow-db:${POSTGRES_PORT:-5432}/${POSTGRES_DB:-airflow}
      # AIRFLOW__CORE__EXECUTOR is already in .env
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      # - ./airflow/config/airflow.cfg:/opt/airflow/airflow.cfg # If using custom config
    ports:
      - "8081:8080" # Mapped to 8081 on host to avoid conflict with Trino's 8080
    command: airflow webserver
    healthcheck:
      test: ["CMD-SHELL", "curl --fail http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    user: "${AIRFLOW_UID:-1000}" # Run as the specified UID

  airflow-scheduler:
    image: apache/airflow:2.8.1
    container_name: airflow-scheduler
    platform: linux/amd64
    restart: unless-stopped
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    networks:
      - data_pipeline_net
    env_file:
      - .env
    environment:
      - AIRFLOW_UID=${AIRFLOW_UID:-1000}
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER:-airflow}:${POSTGRES_PASSWORD:-airflow}@airflow-db:${POSTGRES_PORT:-5432}/${POSTGRES_DB:-airflow}
      # AIRFLOW__CORE__EXECUTOR is already in .env
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      # - ./airflow/config/airflow.cfg:/opt/airflow/config/airflow.cfg # If using custom config
    command: airflow scheduler
    healthcheck:
      # The official healthcheck for scheduler is more involved; this is a basic check
      test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob --hostname \"$$(hostname)\" --limit 1 --allow-multiple || exit 1"]
      interval: 60s
      timeout: 30s
      retries: 5
      start_period: 90s
    user: "${AIRFLOW_UID:-1000}" # Run as the specified UID

volumes:
  minio_data:
  metastore-db:
  clickhouse_data:
  redis_data:
  superset_data:
  airflow_db_data: